# -*- coding: utf-8 -*-
# @Time    : 16/12/19 下午1:50
# @Author  : liulei
# @Brief   :
# @File    : test.py
# @Software: PyCharm Community Edition
#from util import doc_process
from sim_hash import sim_hash
import os
#import jieba
#from util import doc_process
#from util.doc_process import get_postgredb

l = [12530745, 12551171, 1, 2, 3, 4, 5, 6, 7, 8]
#sentence_hash.coll_sentence_hash_time(l)
#conn, cursor = doc_process.get_postgredb()

from bs4 import BeautifulSoup
a = "<h2><a href=\"http://mp.weixin.qq.com/s?__biz=MzIzNTM1MDc5OA==&amp;mid=2247491094&amp;idx=1&amp;sn=237da28c616b6b70546ef68fb439e464&amp;chksm=e8e92077df9ea961cab54fa12b70be94bc27f9bf490a5e5613f107e6b8b1be2b1f4e8f995580&amp;mpshare=1&amp;scene=1&amp;srcid=09274g2Zxp4fI9mXN1wZwEot#rd\"></a></h2>"
b = "<span>看下面这海报，妥妥的迷幻瑰丽，还有着新海诚一向的小清新风格，你们心里有没有痒痒的……不过家居菌决定先不聊这部电影，我们可以先看看这位70后导演的艺术人生……</span>"
c = "诛仙手游普方普泓兑换需要多少经验介绍，诛仙手游普方普泓兑换需要多少经验，诛仙手游阵灵人数随着版本的更新而增加，很多玩家都很喜欢普方普泓，那么拿这两个阵灵需要多少话费呢，小编这里给玩家做个数据分析，喜欢的玩家可借鉴下。"
d =  "<a href=\"http://deeporiginalx.com/search.html#sw=%E8%A7%86%E9%A2%91%E9%93%BE%E6%8E%A5\">视频链接</a>"



e = "同时，下发紧急通知，要求<span><a class=\"a-tips-Article-QQ\" href=\"http://deeporiginalx.com/search.html#sw=%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4\" target=\"_blank\">阿里巴巴</a></span>、腾讯等各重点互联网企业吸取教训，举一反三，采取各项有力措施加强安全防范。尤其要做好全国“<a class=\"a-tips-Article-QQ\" href=\"http://deeporiginalx.com/search.html#sw=%E4%B8%A4%E4%BC%9A\" target=\"_blank\">两会</a>”期间的业务服务保障，确保各类重要系统和业务平台安全稳定运行，为广大网民提供良好的信息服务。"


import datetime
#from multi_viewpoint.sentence_hash import cal_process
nid_set = {12679767,12810510,12754451,12616758,12744618,12742354,12655953,12796591,12587866,12601367,12708499,12756685,12658585,12652855,12833787}
nid_set = {12760123,12586907,12689622,12648243,12586414,12636133,12672578,12663756,12762455,12593306,12804016,12746747,12614355,12690496,12706183}
nid_set = {12950108, 12952268,12801706,12530908,12946157,12950132,12759772,12897222,12759783,12702180,12759785,12699341}
'''
12983194,12969610,12967208,12976815,12948554,12964524,12980360,12966369,12979109,12966023,12978751,12978750,12969235,12966381,12986912,12975154,12978236,12975157,12983572,12984057,12971033,12967936,12961730,12975953,12965164,12965161,12970464,12963889,12976646,12976643,12976642
12971033,12984761,12980230,12978236,1297744,12975424,12984057,12984065,12976643,12979869
12634364,12714558,12709520,12645405,12633599,12603396,12690312,12635403,12750345,12831479,12813605,12661012,12686933,12621899,12600109,12933079,12622641,12661984,12649291,12701882,12677832,12775623,12617199,12647866,12701353,12702051,12711405,12772544,12684973,12682141,12673333,12711237,12711239,12699528,12619964,12710724,12689575,12643726,12720707,12686194,12621096,12661002,12637164,12648848,12711604,12605554,12776139,12657154
12767494,12717653,12581383,12776699,12748362,12776697,12799559,12704230,12574955,12728734,12719049,12715977,12722569,12722568,12587646,12751222,12715978,12751225,12722880,12768525,12600847,12773793,12773790,12792132,12792135,12824054,12581995,12576714,12593367,12615347,12571801,12790633,12788463,12788461,12788466,12746794,12827451,12788468,12799124,12616772,12718623,12792630,12747530,12792633,12792634,12808319,12731261,12819849,12789699,12789696,12789694,12808316,12808313,12748384,12781378,12577533,12766451,12699491,12699495,12930121,12705119,12579409,12705114,12811473,12581391,12587103,12594761,12793516,12755021,12755022,12616316,12788476,12813307,12820175,12744581,12810975,12811870,12930097,12587289,12733158,12768878,12603316,12603317,12603312,12735169,12746172,12735166,12570555,12781388,12781389,12807029,12601379,12601376,12837495,12837490,12837491,12788486,12725997,12765715,12572952,12773297,12773294,12722565,12783474,12783476,12579819,12783472,12798246,12591673,12609833,12809969,12786982,12576701,12768517,12768512,12593814,12740190,12593816,12807326,12571198,12722251,12571205,12716912,12571751,12727944,12666368,12807920,12611800,12602804,12569865,12799117,12930117,12799119,12731862,12805487,12617522,12807359,12572311,12809517,12570854,12739901,12743093,12810997,12581121,12835621,12791191,12791193,12723774,12791196,12776104,12791199,12748005,12820184,12786415,12578661,12767482,12793539,12574485,12769947,12808957,12574962,12606383,12794022,12715966,12607620,12824655,12651943,12821082,12821083,12793535,12572969,12818800,12703744,12572963,12762804,12791206,12592076,12797191,12834974,12775601,12778863,12615357,12735879,12580322,12775177,12798907,12788471,12788470,12777317,12773776,12836259,12570848,12741942,12731276,12768493,12624179,12700754,12587681,12580323,12621873,12582815,12582813,12733667,12724485,12589906,12724480,12738652,12600049,12576723,12811860,12668723,12783742,12930748,12783741,12585068,12802565,12592636,12717455,12810495,12810494,12781392,12781390,12575107,12746166,12781398,12615153,12788940,12837485,12807030,12765097,12836281,12765093,12765090,12931023,12786421,12812360,12818842,12828780,12828074,12720439,12574932,12790360,12615145,12615146,12725202,12756063,12773783,12724478,12722866,12775956,12775954,12740182,12775952,12668224,12732500,12582007,12836699,12836698,12782347,12782349,12824640,12576995,12807915,12837969,12615366,12811858,12790615,12624249,12787679,12793541,12745635,12734507,12731873,12743921,12723367,12731877,12731876,12788935,12723369,12616750,12579993,12736893,12617504,12572981,12798731,12612559,12810981,12735668,12727926,12579425,12570567,12587244,12789706,12610658,12786950,12793533,12786954,12800664,12750830,12750835,12748013,12714100,12786174,12715984,12621460,12621461,12769381,12776306,12769951,12802538,12718308,12570539,12793990,12598987,12733679,12581145,12570849,12835642,12741379,12819848,12570847,12600863,12821090,12821094,12783481,12653677,12836703,12653673,12739897,12797180,12924357,12797185,12793523,12592063,12797764,12720441,12784613,12618095,12592699,12781885,12735884,12609133,12617496,12654629,12787203,12798915,12798912,12578308,12804749,12594061,12578307,12594063,12594067,12783044,12754447,12721629,12754440,12574383,12807307,12721622,12591687,12732496,12794617,12794613,12767906,12602797,12789238,12794589,12729816,12754025,12650971,12650977,12600079,12766989,12784049,12828063,12803375,12784045,12653409,12766982,12733840,12833788,12797783,12581140,12578660,12735631,12591095,12576930,12729617,12835395,12764406,12933952,12931030,12575098,12810974,12748371,12618100,12766977,12834635,12718291,12587097,12790377,12724749,12574941,12616319,12585059,12735853,12722572,12785116,12722879,12834328,12722873,12748931,12792124,12576687,12805500,12576688,12828583,12601673,12726378,12726379,12574389,12591093,12837972,12811845,12593375,12593376,12615170,12743480,12811849,12820681,12781889,12765101,12738179,12785485,12736887,12721613,12577540,12773307,12577544,12577547,12766440,12827686,12602314,12766447,12782819,12579433,12579436,12782813,12767899,12724744,12755355,12798243,12594756,12594755,12835813,12594750,12776120,12785489,12762810,12601669,12621475,12621472,12746810,12725201,12718316,12726385,12733161,12794000,12603321,12781382,12794007,12735173,12585081,12768502,12834986,12834983,12669826,12765707,12773056,12735162,12792694,12653660,12748006,12718000,12718003,12786165,12725195,12782210,12609827,12782215,12581376,12804750,12804753,12578314,12804756,12593803,12593800,12716909,12738193,12624224,12811842,12582845,12589196,12591103,12580301,12580302,12580305,12988995,12569874,12589926,12600061,12594083,12788950,12654636,12570863,12734632,12930722,12802542,12572130,12746786,12577181,12667888,12650982,12796318,12827716,12738693,12835632,12835631,12835635,12812613,12610634,12594757,12722265,12784594,12784595,12578304,12784612
12646114,12971785,12599009,12623680,12749409,12621946,12981195,12968942,12981190,12760228
12931219,12801433,12735407,12820353,12683610,12680304,12735322,12927036,12744933,12732699,12735280,12694001,12686392
12812349,12576678,12578671,12835796,12615365,12593387,12792658,12793548,12642974,12798225,12793544,12776695,12798222,12731870,12834647,12720657,12828081,12767493,12615163,12728807,12734624,12791195,12715970,12751226,12809524,12837973,12668640,12720873,12743485,12790602,12790600,12785478,12835405,12723789,12808958,12608843,12581134,12792137,12786959,12610659,12835823,12988993,12762805,12651432,12776117,12775432,12773304,12773306,12773300,12810503,12766440,12805276,12727389,12663461,12818809,12796594,12809000,12748019,12754461,12798244,12742578,12777485,12834973,12819847,12809528,12766436,12593029,12827456,12615359,12651958,12733680,12765735,12809019,12748434,12808971,12628753,12733160,12601697,12612560,12789310,12807315,12736182,12773778,12755348,12834966,12820661,12828592,12794002,12623777,12781887,12803393,12790349,12773058,12821089,12765705,12803395,12653665,12590332,12653951,12751789,12781385,12735163,12725974,12786424,12738650,12807022,12756024,12655929,12804207,12668721,12804459,12828075,12740452,12991679,12581374,12716916,12734653,12717453,12694439,12769374,12615134,12935902,12666853,12810493,12588597,12580318,12655936,12738645,12754021,12762845,12766051,12574947,12578654,12795564,12803379,12784042,12781400,12797780,12762825,12833784,12731859,12756688,12925872,12790365,12806038,12930100,12810021,12610626,12729799,12788955,12764404,12833772,12576982,12791202,12805481,12807923,12640859,12668225,12796316,12740187,12794041,12798234,12788949,12769377,12834633,12779174,12732509,12582843,12800643,12800642,12837229,12734633,12809514,12790366
12942636,12947360,12963527,12965165,12944807,12743448,12989883,12811869,12953327,12961418
12797390,12728765,12732770,12815019,12661383,12772680,12700828,12740267,12734888,12815034,12698391,12835325,12666617
'''
nid_set = {12988727}
#nid_set = {12942650,12962428,12963527,12942648,12942636,12947903,12987578,12952179,12964666,12788946,12949423,12965165,12940180,12961418,12944807,12953327,12966919,12951641,12952699}
nid_set = {13016327}
#cal_process(nid_set)
#get_pname = "select pname, chid, ctime from newslist_v2 where nid in %s"
#conn, cursor = doc_process.get_postgredb()
#cursor.execute(get_pname, (tuple(nid_set), ))

print sim_hash.is_news_same(17554197, 17568110, 4)
print sim_hash.is_news_same(17554197 ,17704787,  4)
print sim_hash.is_news_same(17568110,17704787,  4)
#13122461

s = "apple芝麻信用涨分宝典，教你快速提升到800芝麻信用，是蚂蚁金服旗下的第三方征信机构，通过云计算、机器学习等技术客观呈现个人的信用状况，" \
    "已经在酒店、租房、租车、出>行、婚恋、分类信息、学生服务、公共事业服务等上万个场景为用户提.供服务。大家都知道信用分高低不同，" \
    "享受的服务待遇也有所不同。蚂蚁金服旗下的借呗，当你的芝麻信用分达到一定的要求，是支持个人消费用途的借款服务，申请之后一般都会及时打" \
    "入用户账户余额。但是广大用户都无法开通蚂蚁借呗，此乃人生一大遗憾，看着大家白花花的银子，自己却无能为力，真是渴望不可及。根据用户反馈，" \
    "芝麻信用分一般在600以上就有60%的机会开通，开通机会是根据支付宝综合评判的结果，所以也有好多人700分了都还没有开通借呗的原因。" \
    "在同等情况>下芝麻信用分550分以上就可以开通“蚂蚁商借”申请2~000到3~0万不等额度，同等速度情况下也是直接打入用户支付宝账户。" \
    "支粉们可以通过微 信 馊.索服务号“蚂蚁商借”进入通道，微信>和陌陌用户均可申请。支付宝借呗和蚂蚁商借年息和日息都相对较低.重点来了，" \
    "我们到底怎么提高芝麻信用分呢？下面主要给大家介绍一些影响芝麻信用分的几个因素，你离涨分只差这几步：1.经常在阿里平台（如淘宝，天猫，" \
    "聚划算，阿里巴巴）上购买商品。2.多使用支付宝给信用卡及时还款，经常使用支付宝在线下商城扫码付款。3.生活服务中多使用支付宝，" \
    "如水>电煤气缴费，租房，看电影，住酒店订机票，打车等。4.信用管理个人信息填写，学历信息，单位邮箱，职业信息，驾驶证，公积金，" \
    "信用卡账单。5.还有就是土豪可以填写海外信用报>告，车辆信息。6.使用信用卡转账，支付宝用户之间转账，保持大金额转账关系，信用分也会增加。" \
    "7.购买理.财产品余额宝或者招财宝之类的产品。8.支付宝添加好友设置星标好友，经常使用支付宝发表动态。综合以上8条，相信大家操作后，芝麻信用分会有所提高"
s2 = '华为P10深度解读：精品智能云生态成为手机竞争新拐点“目前手机高端品牌只有两家，一个是苹果，一个是华为，这已经是事实，华为要在云服务生态体系上全面超越苹果。”华' \
     '为P10上海发布会后接受采访时，华为消费者业务CEO余承东如此说到。虽然华为P10此前已经在巴塞罗那发布过，但这次在国内的上市，华为仍带来了不少“新卖点”，其中之一就是华为已经>打造的安全智能云生态，' \
     '如会上发布的《2016华为消费者云服务白皮书》中所说，2016年，华为应用市场全年累计分发应用450亿次，用户达到2.3亿，华为合作伙伴通过华为消费者云获得的' \
     '分享收益高达 28 亿元人民币，截至2016年底，华为注册开发者数量超过22万，同比上升70%。“华为支付在中国是超越苹果的，”苏杰的一句话点出了华为云生态在中国的接地气，' \
     '但事实上>却道出了华为消费者云的核心打法，就是在全球各大市场都进行真正有效的接地气式的落地，进而让精品智能云生态成为华为手机下一步竞争突破的关键一环。更深一层来看，当诸多手机' \
     '厂商不得不把战略放在性能、外观的比拼时，手机行业的竞争新拐点也出现了，这就是精品智能云生态将成为左右未来手机市场的关键变量，我们不妨从消费者需求来分析一下。<strong>精品' \
     '战略下的安全：安卓手机破局关键之一</strong>安全本身就是精品云服务的根本，“没有安全，何来精品？”苏杰的话其实点中了华为精品智能云生态的核心——安全，而这也是今天安卓手机>破局苹果的一个' \
     '关键要素。众所周知，苹果的iOS系统比之安卓，最大的优势有两个，一个是安全，一个是iCloud服务的方便，这也是很多苹果忠实用户就是不换安卓手机的根本原因。但今>天来看，苹果的安全和云服务也并非是' \
     '铜墙铁壁，此前苹果手机就因为曝出iCloud账号泄露事件而备受用户吐槽。苹果有漏洞，但安卓又能给我们怎样的安全答卷？换言之，在今天安卓阵营对抗苹果之时，安全已经成了安卓系统的最大痛点。' \
     '事实也是如此，据《2016华为消费者云服务白皮书》显示，2016年华为用户越来越关注手机安全，开启“查找我的手机”的用户数相比例2015年上升132%。事实上，从去年开始，在个别国产手机企业的推动下，' \
     '\安全加密芯片也成了竞争中的一大卖点。但只是在安全加密芯片上下功夫，还是从硬件到云服务提供全方位安全？华为做到的正是后者。硬件方面，去年底网信办和工信部公布了2016世界互联网领先科技成果奖，' \
     '华为自主研发的Mate9麒麟960芯片获奖，是手机领域唯一获奖的硬件。麒麟960新增了inSE1.0安全加密芯片，可以保障手机安全，这正是其最大亮点。软件方面，如苏杰在采访中所说，华为是从事通信设备起家，' \
     '安全是产品的基本要求。华为从2011年发力消费者云服务时，就不经意中对标苹果iOS系统和iCloud服务，自然会将保护用户隐私放在最高优先级，参照最严格的欧洲数据保护法要求，在产品设计、' \
     '开发和测试上严格遵循安全隐私设计规范，对用户数据的上传、>下载、存储等各个环节进行多重保护。“国外安全要求非常高，有很严格的法律，安全法规要求，”如余承东所说，华为消费者云在六年多的发展中，还在公司内部还设立了两个部门进行检查' \
     '和监控：一是设立网络安全与用户隐私首席安全官；二是设立一个法务评审环节，包括当地法务和国内法务的要求。2015年，华为云服务通过了全球最权威的CSA-STAR安全认证。这不仅标志' \
     '着华为消费云对用户数据的保护水平达到先进水平，同时也为华为消费者云落地全球市场服务为全球的华为用户打下了基础。“我曾丢过一部华为手机，先远程启动云服务的安全措施删除信>息，然后我发了' \
     '一句话，拾到手机者送还XXX”，在余承东的介绍中，最后这个捡到手机的人不得不把手机还了回来，“因为他根本开不了锁，屏幕上永远只显示‘送还’那句话”，这样的安全保' \
     '密已经做到了“不得不还否则根本无法使用”的程度。“我们的安全设计师是前微软的设计师，提供的安全服务特别高。”余承东的话也是有据可依，华为支付之所以能在中国超越苹果，这其实' \
     '正是明证。不难看出，华为正是通过加密芯片级的硬件加云服务以及全球不同市场的安全级进入，从而形成了高安全级的安卓手机生态，这完全是一个与iOS抗衡的布局，在安卓手机阵营里>，华为的这一打法' \
     '不仅独特而且没有跟随者。再换言之，寰寰姐认为，华为今天所引领的安卓安全服务生态，其实已经形成了安卓阵营破局苹果的重磅武器。华为对安卓产业的引领，值得所' \
     '有国产手机企业思考。<strong>精品服务：人有我强，更要接地气</strong>喜欢看网络小说的寰寰姐最近屡屡看到“外国人沉迷中国网络小说”的新闻，这是中国网民创作力被全球的认同，>但也同时让' \
     '我们看到了中国手机企业提供云服务时的新发展空间。先从最为用户所喜欢的视频来看，娱乐视听是今天手机用户最为关注的，但观看诸多视频App的内容时，有诸多VIP付费、购' \
     '买会员的限制，但华为视频就为华为手机用户提供了35000小时精品片库，包括1500多部好莱坞大片，25000集电视剧，所有内容皆为IMDB及豆瓣中高评分的精品片源。虽然今天诸多手机厂商' \
     '都推出了相应的应用市场服务，有视频、阅读也有支付，表层来看，大家都差不多，但其实深究细里，个中差别巨大。就以华为应用市场来看，华为视频在基础片库层面就全部是无广告高清' \
     '片源，所有会员和非会员都可以享受无广告观影体验，这在国内是首家。不仅超出同行，而且这更符合消费者需求。如《2016华为消费者云服务白皮书》所显示，2016年大热的《速度与激情' \
     '7》、《微微一笑很倾城》都在华为视频里同步无广告高清播放。这么对比来看，连当前竞争激烈的优酷、乐视以及腾讯们所设下的“收费”门槛在华为视频里都不存在了。再看华为阅读，超>过45万册的' \
     '独家和畅销书籍不仅能满足国内消费者，也同样可以让喜欢网络小说的外国读者找到相应的资源，当然这其中还要有智能翻译功能的加持。再看Huawai Pay，目前已经适配超过32家银行，是国内支持银行' \
     '数量最多的移动支付服务，同时支持北上广深的公交地铁卡刷卡。苏杰称，在软件服务方面，对开发者提供全球化的业务能力，以云业务为核心的华为软件生态已经' \
     '成形，但华为的策略是遵循边界意识，专注在基础能力和基础设施上持续投入，不会申请第三方支付牌照，视频不介入内容生产产业，通过合作优质版权方引进优质片源等。这一切的初衷就' \
     '是为华为手机用户提供精品服务——人无我有，人有我精，让消费者云服务真正为手机用户服务。当然，华为消费者云的战略一方面是给消费者带来了独家的云服务优势，但另一方面，不可否' \
     '认的是，这必然也带来手机终端产品在外形、配置竞争之外的“新卖点”，也是华为在这次P10国内发布之时拿出的新武器。<strong>总结：换机时代，精品智能云生态将成决胜关键</strong>六年' \
     '磨一剑，华为的精品云服务其实早已经为华为手机的征战全球提供了有力武器，安全的生态体系、应用市场里的华为视频、华为阅读以及Huawai Pay等都已经为华为手机用户提供了诸多' \
     '后台的基本服务，只是华为没有太过于宣传而己。但为何在这次P10发布会上，消费者云服务成为一大亮点？积累六年当然不只是为了今天手机竞争中的一个“新卖点”，这背后的本质其实还>在于手机市场的变化，' \
     '因为今天已经到了更注重安全、更强调服务的换机时代。据《2016华为消费者云服务白皮书》显示，华为用户换机时更喜欢用手机克隆进行数据传输，使用手机克隆换' \
     '机的用户数相比2015年上升60%。此前，苹果系统的iCloud服务让换机是一件轻松的事，但现在，安卓系统里的华为消费者云服务也做到了。华为其实做了很多事，只是相对低调，当市场有>此需要时，才会大书特书，' \
     '如华为消费者云服务。总结一下，其实不难看出，华为消费者云服务的价值不只是安全、换机等基础云服务这些对标苹果系统的能力，还有更针对中国用户以及全' \
     '球不同市场的落地应用服务，也可以说，精品智能云生态有着华为手机代表安卓阵营去突破苹果优势天花板的重要使命。手机外观颜色的变局只是突显了苹果创新乏力，而此刻六年磨一剑的' \
     '华为消费者云服务的成熟与创新，这不仅是安卓阵营的突围方向，更值得所有安卓手机企业思考"'

def test_ltp():
    from pyltp import Segmentor
    segmentor = Segmentor()
    #segmentor.load('/Users/a000/Downloads/ltp-models/3.3.2/ltp_data.model')
    segmentor.load('/Users/a000/git/ltp_data/cws.model')
    words = segmentor.segment('元芳你怎么看')
    words = segmentor.segment('这本书很好, 我喜欢iphone, 1.5')
    words = segmentor.segment('张子萱怀孕了')
    words = segmentor.segment('我有一本书')
    words = segmentor.segment('今天是2017年3月30日, 清朝的官员')
    words = segmentor.segment('蚂蚁金服近日上市')
    words = segmentor.segment('国家主席习近平抵达美国佛罗里达州')
    words = segmentor.segment('独家|你想要的胸以下全是腿, 科切拉潮人用不')
    total_txt = '<a href=\"http://deeporiginalx.com/search.html#sw=%E7%AC%AC%E4%B8%80%E7%99%BD%E9%93%B6%E7%BD%91\" target=\"_blank\">第一白银网</a>4月19日讯<a href=\"http://deeporiginalx.com/search.html#sw=%E7%8E%B0%E8%B4%A7%E7%99%BD%E9%93%B6\" target=\"_blank\">现货白银</a>今日早盘走势受到美元反弹影响继续走软，目前交投于18.2一线，本周二美国总统特朗普再次提及税改政策，并且宣称将会以“迅雷不及掩耳之势”落地，据小编分析，税改落地将会利好美国经济，从而利好美元，打压白银走势，但问题是，3月份连医改都进展不顺，税改会通过吗?(<a href=\"http://deeporiginalx.com/search.html#sw=%E7%BC%96%E8%BE%91%E6%8E%A8%E8%8D%90%EF%BC%9A%E6%9C%AA%E6%9D%A5%E7%99%BD%E9%93%B6%E8%B5%B0%E5%8A%BF%E5%88%86%E6%9E%90\" target=\"_blank\"><strong><span>编辑推荐：未来白银走势分析</span></strong></a>'
    total_txt =  "<span class=\"article_src\">游民星空</span>2017-04-09<span>阅读原文</span>"
    soup = BeautifulSoup(total_txt, 'lxml')
    total_txt = soup.get_text()
    print total_txt
    print type(total_txt)
    words = segmentor.segment(total_txt.encode('utf-8'))
    #words = segmentor.segment(s)
    for i in words:
        print i

    import jieba
    w_jieba = jieba.cut('独家|你想要的胸以下全是腿, 科切拉潮人用不')
    print '!!!!!'
    for i in w_jieba:
        print i

    from pyltp import Postagger
    poser = Postagger()
    poser.load('/Users/a000/git/ltp_data/pos.model')
    #words_pos = poser.postag(words)
    #for i in xrange(len(words_pos)):
    #    print words[i]
    #    print words_pos[i]


    s1 = '张继科：脚伤恢复七八成 现在不是想退役的时候'
    s2 = '张继科：脚伤恢复八成 现在还不是退役的时候'
    #s2 = '张继科和马龙：脚伤恢复八成 现在还不是退役的时候'
    s3 = '张继科:脚伤已恢复7-8成 现在还不是退役的时候'

    s4= '国际乒联排名：马龙丁宁占据榜首 张继科第四'
    s5 = '国际乒联公布排名：马龙丁宁第一 张继科第四'

    s6 = '国家主席习近平抵达美国佛罗里达州'
    s7 = '习近平抵达美国佛罗里达州'

    s8 = '习近平抵达美国佛罗里达州 同特朗普会晤'
    s9 = '习近平抵达美国佛罗里达州 将与特朗普举行会晤'
    s10 = '习近平抵达美国 将同特朗普举行会晤'
    s11 = '习近平抵达美国佛罗里达州 将同特朗普举行中美元首会晤'

    s12 = '【V观】习近平引用芬兰谚语：没有人的开拓就不会有路'
    s13 = '习近平引用芬兰谚语：没有人的开拓就不会有路'

    s14 = '习近平就圣彼得堡地铁发生爆炸造成伤亡向普京致慰问电' #
    s15 = '习近平就圣彼得堡地铁爆炸事件向普京致慰问电' #15135383
    ss16 = '习近平就圣彼得堡市地铁发生爆炸造成严重人员伤亡向普京致慰问电' #15130013
    ss17 = '习近平就圣彼得堡市地铁爆炸向普京致慰问电' #15127277

    s16 = '习近平离京对芬兰进行国事访问并赴美国举行中美元首会晤' #15131991
    s17 = '习近平离京对芬兰进行国事访问并赴美举行中美元首会晤' #15132864
    s18 = '习近平离京对芬兰共和国进行国事访问并赴美国佛罗里达州举行中美元首会晤' #15131971
    ws1  = segmentor.segment(s6)
    ws2  = segmentor.segment(s7)
    print '  '.join(ws1)
    print '  '.join(ws2)
    pos1 = poser.postag(ws1)
    pos2 = poser.postag(ws2)
    print ' '.join(pos1)
    print ' '.join(pos2)

    from pyltp import NamedEntityRecognizer
    reco = NamedEntityRecognizer()
    reco.load('/Users/a000/git/ltp_data/ner.model')
    ne1 = reco.recognize(ws1, pos1)
    ne2 = reco.recognize(ws2, pos2)
    print ' '.join(ne1)
    print ' '.join(ne2)

    from pyltp import Parser
    parser = Parser()
    parser.load('/Users/a000/git/ltp_data/parser.model')
    arc1 = parser.parse(ws1, pos1)
    arc2 = parser.parse(ws2, pos2)
    print ' '.join("%d:%s" %(arc.head, arc.relation) for arc in arc1)
    print ' '.join("%d:%s" %(arc.head, arc.relation) for arc in arc2)






from util.langconv import *

def Traditional2Simplified(sentence):
    '''
    将sentence中的繁体字转为简体字
    :param sentence: 待转换的句子
    :return: 将句子中繁体字转换为简体字之后的句子
    '''
    sentence = Converter('zh-hans').convert(sentence)
    return sentence

def Simplified2Traditional(sentence):
    '''
    将sentence中的简体字转为繁体字
    :param sentence: 待转换的句子
    :return: 将句子中简体字转换为繁体字之后的句子
    '''
    sentence = Converter('zh-hant').convert(sentence)
    return sentence

def test_special_space():
    from util.doc_process import get_postgredb_query
    sql = "select title, content from newslist_v2 where nid = 13282986"
    conn, cursor = get_postgredb_query()
    cursor.execute(sql)
    rows = cursor.fetchall()
    for row in rows:
        title = row[0]
        content_list = row[1]
        txt = ''
        for content in content_list:
            if 'txt' in content.keys():
                txt += content['txt'] + ' '   #unicode

        soup = BeautifulSoup(txt, 'lxml')
        txt = soup.get_text()
        total_txt = title + ' ' + txt.encode('utf-8')
        print total_txt
        total_txt = ''.join(total_txt.split())

    print total_txt
    total_txt = total_txt.replace('\xe2\x80\x8b', '')
    total_txt = total_txt.replace('\xe2\x80\x8c', '')
    total_txt = total_txt.replace('\xe2\x80\x8d', '')
    from pyltp import Postagger
    poser = Postagger()
    poser.load('/Users/a000/git/ltp_data/pos.model')
    from pyltp import Segmentor
    segmentor = Segmentor()
    segmentor.load('/Users/a000/git/ltp_data/cws.model')
    ws = segmentor.segment(total_txt)
    wspos = poser.postag(ws)
    for k, i in enumerate(wspos):
        print ws[k]
        print i
        if k > 300:
            break

def pool_sleep(n):
    import time
    print '{}   -----begin'.format(n)
    time.sleep(n)
    print 'sleep : ' + str(n)

import requests
import json
from util.doc_process import get_postgredb
#生成专题
def generate_subject2(sub_nids):
    prefix = 'http://fez.deeporiginalx.com:9001'
    create_url = prefix + '/topics'
    cookie = {'Authorization':'f76f3276c1ac832b935163c451f62a2abf5b253c'}
    #set subject name as one title of one piece of news
    sql = "select title from newslist_v2 where nid=%s"
    conn, cursor = get_postgredb()
    cursor.execute(sql, (sub_nids[0],))
    row = cursor.fetchone()
    sub_name = row[0]
    data = {'name':sub_name}
    response = requests.post(create_url, data=data, cookies=cookie)
    print response.content
    content = json.loads(response.content)
    id = content['id']

    topic_class_url = prefix + '/topic_classes'
    data = {'topic': id, 'name': 'random'}
    response = requests.post(topic_class_url, data=data, cookies=cookie)
    class_id = json.loads(response.content)['id']

    add_nid_url = prefix + '/topic_news'
    for nid in sub_nids:
        data = {'topic_id':id, 'news_id':nid, 'topic_class_id':class_id}
        requests.post(add_nid_url, data=data, cookies=cookie)


def test_jsonb():
    conn, cursor = get_postgredb()
    id=0
    sentence = ['测试句子1', '测试句子2']
    insert_sql = "insert into topic_sentences (topic_id, sentences) values (%s, %s)"
    print json.dumps(sentence)
    #cursor.execute(insert_sql, (id, json.dumps(sentence)))
    #conn.commit()
    query_sql = "select sentences from topic_sentences where topic_id=%s"
    cursor.execute(query_sql, (id, ))
    row = cursor.fetchone()
    print row[0]
    print type(row[0])
    conn.close()

if __name__=="__main__":
    from util.doc_process import coll_cut_extract, get_file_real_dir_path
    import traceback
    try:
        chnl_k_dict2 = {'财经':20, '股票':10, '故事':20, '互联网':20, '健康':30, '军事':20}
        save_dir =os.path.split(os.path.realpath(__file__))[0]
        print type(save_dir)
        idf_path = os.path.join(save_dir, 'idf_test.txt')
        print type(idf_path)
        print idf_path

        test = {'a':[1, 2], 'b':[3, 4]}
        import pandas as pd
        #df = pd.DataFrame(test, columns=('a', 'b'))
        #df.to_csv(idf_path, index=False)
        #coll_cut_extract(chnl_k_dict2, save_dir, idf_save_path=idf_path)
        #test_ltp()
        sss = {'a':2, 'b':2, 'c':5, 'd':5}
        #from multi_viewpoint import sentence_hash
        a = {16662232,}
        #sentence_hash.cal_process(a)

        '''
        import requests
        prefix = 'http://fez.deeporiginalx.com:9001'
        #prefix = 'http://fez.deeporiginalx.com:9001/pandect'
        create_url = prefix + '/topics'
        print create_url
        #headers = {'Cookies':{'Authorization':'Token cabe590238ec318f84454ff1169f2dc259606b2b'}}
        cookie = {'Authorization':'f76f3276c1ac832b935163c451f62a2abf5b253c'}
        header = {'Cookie':cookie}
        data = {'name':'test22ss'}
        response = requests.post(create_url, data=data, cookies=cookie)
        #response = requests.post(create_url, data=data, headers=header)
        #print type(response.content)
        print response.content
        import json
        content = json.loads(response.content)
        print content
        id = content['id']
        print id

        topic_class_url = prefix + '/topic_classes'
        data = {'topic':id, 'name':'random'}
        response = requests.post(topic_class_url, data=data, cookies=cookie)
        print response.content
        class_id = json.loads(response.content)['id']
        print class_id



        add_nid_url = prefix + '/topic_news'
        data = {'topic_id':id, 'news_id':16792033, 'topic_class_id':class_id}
        response = requests.post(add_nid_url, data=data, cookies=cookie)
        print response.content
        '''
        #from multi_viewpoint.sentence_hash import generate_subject
        #generate_subject((16662232, 16795873))
        #import pandas_test
        #test_jsonb()

        print 'fffffffff'

    except:
        traceback.print_exc()


    #traditional_sentence = '憂郁的臺灣烏龜'
    #simplified_sentence = Traditional2Simplified(traditional_sentence.decode('utf-8'))
    #print(simplified_sentence)

    '''
    import jieba.posseg as pseg
    import jieba
    jieba.load_userdict("util/networds.txt")
    #s = '大家喜欢apple, 一只小狗, 需要今日清朝'
    s = '长得十分漂亮, 跑的很快'
    w = pseg.cut(s)
    #wp = {}
    for i in w:
        print i
        #wp[i[0]] = i[1]
        #print i[0]

    #allow_pos = ('v', 'vd', 'vn', 'vg', 'vx', 'vi', 'vl', 'vf', 'vshi', 'vyou')
    #allow_pos = ('v',)
    allow_pos = ("/eng")
    #allow_pos = ('a', 'ad', 'an','ag', 'al', 'f', 'g', 'n', 'nr', 'ns', 'nt', 'ng', 'nl','nz'
    allow_pos =  ('t', 's', 'v', 'vd', 'vg', 'vl', 'vi', 'vx', 'vf', 'vn', 'z', 'i', 'j', 'l', 'eng')
    allow_pos = ('z')
    allow_pos = ('a', 'n', 'v', 'eng', 's', 't', 'i', 'j', 'l', 'z')
    allow_pos = ('n', 'nr')
    print '------------'
    tags = jieba.analyse.extract_tags(s, topK=1000,
                                      withWeight=False, allowPOS=allow_pos)
    '''
    '''
    from util import doc_process
    from util.simhash import simhash, get_4_segments

    w1 = doc_process.get_words_on_nid(15376954)
    print ' '.join(w1)
    #w2 = doc_process.get_words_on_nid(15134277)
    #h1 = simhash(ws1)
    #h2 = simhash(ws2)
    #print h1.hamming_distance(h2)
    s_new = "select nid from newslist_v2 where ctime > now() - interval '30 day' and chid not in (28, 23, 21, 44) and state=0 limit 5"
    from util import doc_process

    conn, cursor = doc_process.get_postgredb_query()
    cursor.execute(s_new)
    nids = cursor.fetchall()
    print type(nids)
    a = nids[0:2]
    print a
    for i in nids:
        print i[0]
    '''



# ，/x,
# vn: 信用 芝麻 蚂蚁 用户 开通 转账 信用卡 金服 使用 信息 大家 租房 申请 填写 打入 好友 余额 酒店 订机票 账户 日息 旗下 支粉 涨分 扫码 天猫 星标 离涨 购买 婚恋 租车 年息 个人信息 征信 公共事业 机会 邮箱 账单 土豪 财宝 划算 缴费 产品 打车 学历 付款 煤气 还款 分会 个人消费 公积金 反馈 场景 添加 情况 借款 渴望 待遇 用途 客观 人生 职业 动态 额度 电影 机器 通道 分类 银子 享受 车辆 平台 金额 呈现 状况 学习 学生 发表 相信 计算 提升 速度 介绍 操作 不可 看着 个人 无法 单位 商品 重点 支持 原因 结果 因素 保持 关系 提高 技术 机构 要求 增加 进入 达到 还有 知道 没有
# vd: 开通 转账 使用 申请 填写 打入 经常 离涨 购买 征信 划算 缴费 打车 付款 还款 反馈 添加 渴望 通道 享受 呈现 到底 学习 快速 发表 真是 相信 计算 提升 介绍 操作 不可 看着 支持 相对 保持 提高 要求 增加 进入 达到 一定 还有 知道 就是 已经 没有
# n:  信用 芝麻 蚂蚁 用户 信用卡 金服 信息 大家 租房 好友 余额 酒店 订机票 账户 日息 旗下 天猫 星标 支粉 扫码 涨分 婚恋 租车 年息 个人信息 公共事业 机会 邮箱 账单 土豪 财宝 产品 学历 煤气 分会 个人消费 公积金 场景 情况 借款 待遇 用途 客观 人生 职业 动态 额度 电影 机器 分类 银子 车辆 平台 金额 状况 学生 速度 个人 无法 单位 商品 重点 原因 结果 因素 关系 技术 机构
# vd:开通 转账 使用 申请 填写 打入 离涨 购买 征信 划算 缴费 打车 付款 还款 反馈 添加 渴望 通道 享受 呈现 学习 发表 相信 计算 提升 介绍 操作 不可 看着 支持 保持 提高 要求 增加 进入 达到 还有 知道 没有
#  d: 经常 到底 快速 真是 相对 一定 就是 已经
#  g:
#  j:
#  l有所提高 无能为力 有所不同:
#  i:
#  q:
#  c: 及时 可以 或者 所以 但是
#  e:
#  f: 以上 下面 之间 之后 北上 背后 之外 最近 内部 最后
#  h:
#  m: 2. 800 1. 上万个 第三方
#  'a':一般 遗憾 高低 一大 广大 不同
#  'ad':经常 一般 遗憾 高低 一大 广大 到底 快速 真是 相对 不同 一定 就是 已经
#   'z':


